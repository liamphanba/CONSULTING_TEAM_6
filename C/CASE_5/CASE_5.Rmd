---
title: "CASE_3"
author: "Liam Phan, Michael Bigler, Tania Loureiro, William Elkiess, Dakota Cuellar and Ilyana El Mendili"
date: "`r Sys.Date()`"
output: 
  rmdformats::downcute:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE,comment = FALSE, error = FALSE,options(scipen=999))
rm(list = ls())
```

# Packages

```{r packages}
library(DT)
library(pROC)
library(summarytools)
library(corrplot)
library(dplyr)
library(GGally)
library(fastDummies)
library(ggcorrplot)
library(klaR)
library(psych)
library(MASS)
# library(ggord)
library(devtools)
library(ggplot2)
library(ggthemes)
library(GGally)
library(caret)
library(splitTools)
library(rpart)
library(xgboost)
library(caTools)
library(dplyr)
library(caret)
library(naniar)

CM_Function <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#2F4F4E')
  text(195, 435, 'No', cex=1.2)
  rect(250, 430, 340, 370, col='#0D8387')
  text(295, 435, 'Yes', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#0D8387')
  rect(250, 305, 340, 365, col='#2F4F4E')
  text(140, 400, 'No', cex=1.2, srt=90)
  text(140, 335, 'Yes', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  

#options(repos = c(
    #fawda123 = 'https://fawda123.r-universe.dev',
    #CRAN = 'https://cloud.r-project.org'))

# Install ggord
#install.packages('ggord')

```

# Data and Feature Engineering

```{r data loading}

df <- readxl::read_xls('Cchurn.xls')
df$international_plan <- factor(df$international_plan, levels = c('no', 'yes'), labels = c('0','1'))
df$voice_mail_plan <- factor(df$voice_mail_plan, levels = c('no', 'yes'), labels = c('0','1'))
df$churn <- factor(df$churn, levels = c('no', 'yes'), labels = c('0','1'))

```

## Summary

```{r summary}
print(summarytools::dfSummary(df), method = 'render')
```

* We have no missing values -> perfect
* Heavily uneven counts of dependent variable (86 % no / 14 % yes) -> maybe sample for equality / maybe not because we loose information of other data
* Independent variables are on different scales -> standardize
* two (maybe three) categorical predictors: International plan / voice_mail_plan (/ maybe number_customer_service_calls) -> dummy encode -> not necessary as already 0 and 1
* Rest of data is numeric and most of the variables looks normally distributed with exception of number_vmail_messages and totat_intl_calls
  * transform these value to make them normal?
  * maybe make parts of them categorical? (recieving voice mail or not, calling internationally or not)
  * or maybe the categorical values that we have already give an indication for this
  * Test normality of variables
* Can variables be combined? We have day / eve / night / intl calls and for each of them minutes / calls / charge. Maybe we can combine this into one metric. Maybe average cost per minute or average cost per call? 

## Correlation Plot

```{r correlation}

df_numeric <- select_if(df, is.numeric)  # Subset numeric columns with dplyr

M <- cor(df_numeric)

p.mat <- cor_pmat(df_numeric)

ggcorrplot(M, hc.order = TRUE, type = "lower", lab = TRUE, p.mat = p.mat, sig.level=0.05, lab_size = 2, tl.cex = 10,outline.col = "white", ggtheme = ggplot2::theme_minimal(), colors = c("#823038", "white", "#2596be")) 

```

Proves theory from before -> we can make one metric out of charge and minutes --> charge / minutes

## Data Engineering

```{r transform metrics}
df$total_day_charge_per_minute <- ifelse(df$total_day_minutes == 0, 0, df$total_day_charge / df$total_day_minutes)
df$total_eve_charge_per_minute <- ifelse(df$total_eve_minutes == 0, 0, df$total_eve_charge / df$total_eve_minutes)
df$total_night_charge_per_minute <- ifelse(df$total_night_minutes == 0, 0, df$total_night_charge / df$total_night_minutes)
df$total_intl_charge_per_minute <- ifelse(df$total_intl_minutes == 0, 0, df$total_intl_charge / df$total_intl_minutes)
df <- subset(df, select = -c(total_day_charge, total_day_minutes, total_eve_charge, total_eve_minutes, total_night_charge, total_night_minutes, total_intl_charge, total_intl_minutes))
```

## Correlation Plot

```{r correlation 2}

df_numeric <- select_if(df, is.numeric)  # Subset numeric columns with dplyr

M <- cor(df_numeric)

p.mat <- cor_pmat(df_numeric)

ggcorrplot(M, hc.order = TRUE, type = "lower", lab = TRUE, p.mat = p.mat, sig.level=0.05, lab_size = 2, tl.cex = 10,outline.col = "white", ggtheme = ggplot2::theme_minimal(), colors = c("#823038", "white", "#2596be")) 

```

Now we have non-correlated data

## Relationship between variables

```{r pairs plot}
# theme_set(theme_minimal())
# 
# ggpairs(
#   data = df,
#   columns = c(1:9,11:14),
#   mapping = aes(col = churn, alpha = .9)
# ) +
#   scale_fill_colorblind() +
#   scale_color_colorblind()
```

We see that data is hard to seperate linearly between the classes. Therefore one can introduce new features of higher order or use methods which do not need the data to be seperable linearly. 

## Adding features of higher order

Only squaring as we have no negative data. Cubing would be needed with negative data. 

```{r adding features}
# squared
df2 <- df^2
df2 <- df2[,-c(2,3,10)]
colnames(df2) <- paste0(colnames(df2), '_sqd')

df <- cbind(df,df2)
```

## Relationship between data in higher order

```{r higher order rel}
# theme_set(theme_minimal())
# 
# ggpairs(
#   data = df,
#   columns = c(1:9, 11:25),
#   mapping = aes(col = churn, alpha = .9)
# ) +
#   scale_fill_colorblind() +
#   scale_color_colorblind()
```

## Sampling Methods

As we have unbalanced data we need to use a sampling method to balance the classes. Hereby there are four different methods. OVER / UNDER / BOTH / ROSE. 

```{r}

library(ROSE)

# OVER
df_OVER <- ovun.sample(churn~., data = df, method = "over")$data

table(df$churn)
table(df_OVER$churn)

# UNDER
df_UNDER <- ovun.sample(churn~., data = df, method = "under")$data

table(df$churn)
table(df_UNDER$churn)

# BOTH
df_BOTH <- ovun.sample(churn~., data = df, method = "both")$data

table(df$churn)
table(df_BOTH$churn)

# ROSE
df_ROSE <- ROSE(churn ~ ., data = df, seed = 1, p = 0.5)$data

```

## Sampling Visualization

```{r}

# theme_set(theme_minimal())
# 
# ggpairs(
#   data = df_ROSE,
#   columns = c(1:9, 11:25),
#   mapping = aes(col = churn, alpha = .9)
# ) +
#   scale_fill_colorblind() +
#   scale_color_colorblind() +
#   labs(title = "Machine Learning Project")
# 
# ggpairs(
#   data = df_OVER,
#   columns = c(1:9, 11:25),
#   mapping = aes(col = churn, alpha = .9)
# ) +
#   scale_fill_colorblind() +
#   scale_color_colorblind() +
#   labs(title = "Machine Learning Project")

# ggpairs(
#   data = df_UNDER,
#   columns = c(1:9, 11:25),
#   mapping = aes(col = churn, alpha = .9)
# ) +
#   scale_fill_colorblind() +
#   scale_color_colorblind() +
#   labs(title = "Machine Learning Project")

# ggpairs(
#   data = df_BOTH,
#   columns = c(1:9, 11:25),
#   mapping = aes(col = churn, alpha = .9)
# ) +
#   scale_fill_colorblind() +
#   scale_color_colorblind() +
#   labs(title = "Machine Learning Project")
```

## Train / Test split

As we need to test the models we need to split the sampled data. 

```{r train test split}
set.seed(1)
data <- df_OVER # choose which data to use df_ROSE / df_BOTH / df_UNDER / df_OVER / df
inds <- splitTools::partition(data$churn, p = c(train = 0.7, test = 0.3))
dftrain <- df[inds$train,]
dftest <- df[inds$test,]
```

## Standardizing

As some methods need scaled data we scale the data here to be centered. 

```{r standardize}
norm.value <- preProcess(dftrain, method = c("center", "scale"))
dftrain <- predict(norm.value, dftrain)
dftest <- predict(norm.value, dftest)
```

# Predicting Models

## QDA

````{r qda}

mod.qda <- qda(churn ~., data = dftrain)

predicted.qda <- predict(mod.qda, dftest[,-c(10)])$class

confmat.qda <- confusionMatrix(data=predicted.qda, reference = dftest$churn, positive = '1')

CM_Function(confmat.qda)

roc_score.qda =roc(factor(dftest$churn, ordered=TRUE), factor(predicted.qda, ordered=TRUE))
plot(roc_score.qda ,main ="ROC curve")

```

## QLOG

````{r qda}

mod.log <- glm(churn ~., data = dftrain, family = binomial(link = "probit"))

s <- step(mod.log)

mod.log <- glm(s$formula, data = dftrain, family = binomial(link = "probit"))

predicted.log <- factor(ifelse(predict(mod.log, dftest[,-c(10)], type='response')>0.13,1,0))

confmat.log <- confusionMatrix(data=predicted.log, reference = dftest$churn, positive = '1')

CM_Function(confmat.log)

roc_score.log =roc(factor(dftest$churn, ordered=TRUE), factor(predicted.log, ordered=TRUE))
plot(roc_score.log ,main ="ROC curve")

```

## Boosting 

```{r}
# library(gbm)
# 
# mod <-  gbm(churn ~.,
#                 data = dftrain,
#                 distribution = "gaussian",
#                 cv.folds = 10,
#                 shrinkage = .01,
#                 n.minobsinnode = 10,
#                 n.trees = 500)
# 
# predicted <- factor(ifelse(1/(1+exp(-2*predict.gbm(mod, dftest[,-c(10)])))>=0.5,1,0))
# 
# 
# dftrain <-  dftrain |>
#   mutate_if(is.factor, as.character) |>
#   mutate_if(is.character, as.numeric)
# 
# dftest <-  dftest |>
#   mutate_if(is.factor, as.character) |>
#   mutate_if(is.character, as.numeric)
# 
# xgb_train <- xgb.DMatrix(data = as.matrix(dftrain[,-c(10)]), label = dftrain$churn)
# xgb_test <- xgb.DMatrix(data = as.matrix(dftest[,-c(10)]), label = dftest$churn)
# xgb_params <- list(
#   booster = "gbtree",
#   eta = 0.01,
#   max_depth = 8,
#   gamma = 4,
#   subsample = 0.75,
#   colsample_bytree = 1,
#   objective = "multi:softprob",
#   eval_metric = "mlogloss",
#   num_class = 2)
# 
# xgb_model <- xgb.train(
#   params = xgb_params,
#   data = xgb_train,
#   nrounds = 100,
#   verbose = 1
# )
# 
# xgb_model
# 
# xgb_preds <- predict(xgb_model, as.matrix(dftest$churn), reshape = TRUE)
# xgb_preds <- as.data.frame(xgb_preds)
# colnames(xgb_preds) <- c(0,1)
# predicted <- ifelse(xgb_preds[,2] > 0.5, 1, 0)
# 
# CM_Function(confusionMatrix(data=predicted, reference = dftest$churn, positive = "1"))
# 
# library(pROC)
# roc_score=roc(factor(dftest$churn, ordered=TRUE), factor(predicted, ordered=TRUE)) #AUC score
# plot(roc_score ,main ="ROC curve")
```

## Bagging 

```{r bagging}
set.seed(123)
library(ipred)

# train bagged model
ames_bag1 <- bagging(
  formula = churn ~ .,
  data = dftrain, 
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
  )

ames_bag1

predicted <- factor(ifelse(predict(ames_bag1, dftest[,-c(10)], type = 'prob')[,2] >= 0.2, 'yes', 'no'))
                    
CM_Function(confusionMatrix(data=predicted, reference = dftest$churn, positive = 'yes'))

roc_score=roc(factor(dftest$churn, ordered=TRUE), factor(predicted, ordered=TRUE)) #AUC score
plot(roc_score ,main ="ROC curve")

```




##  Logistic Regression

```{r predict}

mod <- glm(churn ~., data = df_UNDER, family = binomial(link='logit'))
summary(mod)

churn_predicted <- factor(ifelse(predict(mod, df[,-c(10)], type = 'response') < 0.5, 'no', 'yes'))

library(caret)

LR_Confusion_Matrix_training <- confusionMatrix(data=churn_predicted, reference = df$churn, positive = 'yes')

CM_Function(LR_Confusion_Matrix_training)

```

Log regression doesn't work. Changing the decision boundary also doesn't help. --> maybe we actually need to resample

https://www.r-bloggers.com/2021/05/class-imbalance-handling-imbalanced-data-in-r/
https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/
https://datascientistdiary.com/index.php/2021/09/02/how-to-handle-imbalanced-data-example-in-r/
https://cran.r-project.org/web/packages/imbalance/vignettes/imbalance.pdf


## LDA 

```{r LDA}

# LDA Model Fit on Training
LDA_training <- lda(churn~., df_UNDER)
LDA_training

# Density Plot for Overlapping 
p <- predict(LDA_training, df)
ldahist(data = p$x[,1], g = df$churn, col = "#0D8387")

```

### Confusion Matrix (Training VS Test)

```{r}

# Training 60%
LDA_predictions_training <- predict(LDA_training, df)$class

LDA_Confusion_Matrix_training <- confusionMatrix(data = LDA_predictions_training, reference = df$churn, positive='yes')

CM_Function(LDA_Confusion_Matrix_training)

```

## Random Forest 

Best with ROSE sampling

```{r}
library(randomForest)
library(caret)
set.seed(1)

# Set up cross-validation
control <- trainControl(method = "cv", number = 5, savePredictions = "final", classProbs = TRUE)

# Train the Random Forest model using cross-validation
model_cv <- train(churn ~ ., data = df_ROSE, method = "rf", trControl = control, ntree = 500, importance = TRUE)

# Model performance (average metrics across folds)
model_cv$results

# Confusion Matrix and performance metrics (based on all the cross-validated predictions)
confusionMatrix(model_cv$pred$pred, model_cv$pred$obs, positive = 'yes')
CM_Function(confusionMatrix(model_cv$pred$pred, model_cv$pred$obs, positive = 'yes'))

# Fit the Random Forest model with the optimal mtry value
RF_optimal <- randomForest(churn ~ ., data = df_ROSE, mtry = 6, ntree = 500)

# Mean Decrease Gini
importance <- importance(RF_optimal)
importance


```


## Random Forest V2

```{r}

# ROSE DATA

rf <- randomForest(churn~., data=df_UNDER, proximity=TRUE)

Predictions_OVER <- predict(rf, df_UNDER[,-10])

CM_OVER <- confusionMatrix(Predictions_OVER, df_UNDER$churn, positive = 'yes')

CM_Function(CM_OVER)

# ORIGINAL DATA

Predictions_DATA <- predict(rf, df[,-10])

CM_DATA <- confusionMatrix(Predictions_DATA, df$churn, positive = 'yes')

CM_Function(CM_DATA)

```


## Decision Trees

```{r}

tree <- rpart(churn ~., data = df)

rpart.plot(tree)

printcp(tree)
plotcp(tree)

Predictions_DT <- predict(tree, df[,-10])[,2]

Predictions_DT <- ifelse(Predictions_DT > 0.5, "yes","no")

Predictions_DT <- as.factor(Predictions_DT)

CM_DATA <- confusionMatrix(Predictions_DT, df$churn, positive = 'yes')

CM_Function(CM_DATA)

```

## Neural Net

```{r neural}

library(caret)

nn1 <- train(churn ~ ., data = df_UNDER, method = "nnet")

nn1.pre <- predict(nn1, df[,-c(10)])

confusionMatrix(nn1.pre, df$churn, positive = 'yes')
CM_Function(confusionMatrix(nn1.pre, df$churn, positive = 'yes'))

```


## Support Vector Machines

```{r}

library(e1071)

classifierR = svm(formula = churn ~ .,
                 data = df_UNDER,
                 type = 'C-classification', # this is because we want to make a regression classification
                 kernel = 'radial')

svm_1 <- predict(classifierR, df[,-c(10)])

confusionMatrix(svm_1, df$churn, positive = 'yes')

CM_Function(confusionMatrix(svm_1, df$churn, positive = 'yes'))

```