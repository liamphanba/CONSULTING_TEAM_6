---
title: "CASE_3"
author: "Liam Phan, Michael Bigler, Tania Loureiro, William Elkiess, Dakota Cuellar and Ilyana El Mendili"
date: "`r Sys.Date()`"
output: 
  rmdformats::downcute:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE,comment = FALSE, error = FALSE,options(scipen=999))
rm(list = ls())
```

```{r packages}
library(DT)
library(summarytools)
library(corrplot)
library(dplyr)
library(GGally)
library(fastDummies)
library(ggcorrplot)
library(klaR)
library(psych)
library(MASS)
library(ggord)
library(devtools)

CM_Function <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#2F4F4E')
  text(195, 435, 'No', cex=1.2)
  rect(250, 430, 340, 370, col='#0D8387')
  text(295, 435, 'Yes', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#0D8387')
  rect(250, 305, 340, 365, col='#2F4F4E')
  text(140, 400, 'No', cex=1.2, srt=90)
  text(140, 335, 'Yes', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  

#options(repos = c(
    #fawda123 = 'https://fawda123.r-universe.dev',
    #CRAN = 'https://cloud.r-project.org'))

# Install ggord
#install.packages('ggord')

```

# Data and Feature Engineering

```{r data loading}

df <- readxl::read_xls('Cchurn.xls')
df$international_plan <- factor(df$international_plan, levels = c('no', 'yes'))
df$voice_mail_plan <- factor(df$voice_mail_plan, levels = c('no', 'yes'))
df$churn <- factor(df$churn, levels = c('no', 'yes'))

```

## Summary

```{r summary}
print(summarytools::dfSummary(df), method = 'render')
```

* We have no missing values -> perfect
* Heavily uneven counts of dependent variable (86 % no / 14 % yes) -> maybe sample for equality / maybe not because we loose information of other data
* Independent variables are on different scales -> standardize
* two (maybe three) categorical predictors: International plan / voice_mail_plan (/ maybe number_customer_service_calls) -> dummy encode -> not necessary as already 0 and 1
* Rest of data is numeric and most of the variables looks normally distributed with exception of number_vmail_messages and totat_intl_calls
  * transform these value to make them normal?
  * maybe make parts of them categorical? (recieving voice mail or not, calling internationally or not)
  * or maybe the categorical values that we have already give an indication for this
  * Test normality of variables
* Can variables be combined? We have day / eve / night / intl calls and for each of them minutes / calls / charge. Maybe we can combine this into one metric. Maybe average cost per minute or average cost per call? 

## Correlation Plot

```{r correlation}

df_numeric <- select_if(df, is.numeric)  # Subset numeric columns with dplyr

M <- cor(df_numeric)

p.mat <- cor_pmat(df_numeric)

ggcorrplot(M, hc.order = TRUE, type = "lower", lab = TRUE, p.mat = p.mat, sig.level=0.05, lab_size = 2, tl.cex = 10,outline.col = "white", ggtheme = ggplot2::theme_minimal(), colors = c("#823038", "white", "#2596be")) 

```

Proves theory from before -> we can make one metric out of charge and minutes --> charge / minutes

## Data Engineering

```{r transform metrics}
df$total_day_charge_per_minute <- ifelse(df$total_day_minutes == 0, 0, df$total_day_charge / df$total_day_minutes)
df$total_eve_charge_per_minute <- ifelse(df$total_eve_minutes == 0, 0, df$total_eve_charge / df$total_eve_minutes)
df$total_night_charge_per_minute <- ifelse(df$total_night_minutes == 0, 0, df$total_night_charge / df$total_night_minutes)
df$total_intl_charge_per_minute <- ifelse(df$total_intl_minutes == 0, 0, df$total_intl_charge / df$total_intl_minutes)
df <- subset(df, select = -c(total_day_charge, total_day_minutes, total_eve_charge, total_eve_minutes, total_night_charge, total_night_minutes, total_intl_charge, total_intl_minutes))
```

## Correlation Plot

```{r correlation 2}

df_numeric <- select_if(df, is.numeric)  # Subset numeric columns with dplyr

M <- cor(df_numeric)

p.mat <- cor_pmat(df_numeric)

ggcorrplot(M, hc.order = TRUE, type = "lower", lab = TRUE, p.mat = p.mat, sig.level=0.05, lab_size = 2, tl.cex = 10,outline.col = "white", ggtheme = ggplot2::theme_minimal(), colors = c("#823038", "white", "#2596be")) 

```

Now we have non-correlated data

## Scale

```{r standardize}

df <- df %>% mutate_if(~!is.factor(.), ~(scale(.) %>% as.vector))

```


# Predicting Models

##  Logistic Regression

```{r predict}

mod <- glm(churn ~., data = df, family = binomial(link='logit'))
summary(mod)

churn_predicted <- factor(ifelse(predict(mod, df[,-c(10)], type = 'response') < 0.5, 'no', 'yes'))

library(caret)

LR_Confusion_Matrix_training <- confusionMatrix(data=churn_predicted, reference = df$churn, positive = 'yes')

CM_Function(LR_Confusion_Matrix_training)

```

Log regression doesn't work. Changing the decision boundary also doesn't help. --> maybe we actually need to resample

## LDA 

```{r LDA}

set.seed(1)

# Row Sample
ind <- sample(2, nrow(df),
              replace = TRUE,
              prob = c(0.6, 0.4))

# Training 60%
training <- df[ind==1,]

# Testing 40%
testing <- df[ind==2,]

# LDA Model Fit on Training
LDA_training <- lda(churn~., training)
LDA_training

# Density Plot for Overlapping 
p <- predict(LDA_training, training)
ldahist(data = p$x[,1], g = training$churn, col = "#0D8387")

```

### Confusion Matrix (Training VS Test)

```{r}

# Training 60%
LDA_predictions_training <- predict(LDA_training, training)$class

LDA_Confusion_Matrix_training <- confusionMatrix(data = LDA_predictions_training, reference = training$churn)

CM_Function(LDA_Confusion_Matrix_training)

# Testing 40%
LDA_predictions_testing <- predict(LDA_training, testing)$class

LDA_Confusion_Matrix_testing <- confusionMatrix(data = LDA_predictions_testing, reference = testing$churn,positive = 'yes')

CM_Function(LDA_Confusion_Matrix_testing)

```

## Random Forest

```{r}
library(randomForest)
library(caret)

set.seed(1)

# Set up cross-validation
control <- trainControl(method = "cv", number = 5, savePredictions = "final", classProbs = TRUE)

# Train the Random Forest model using cross-validation
model_cv <- train(churn ~ ., data = df, method = "rf", trControl = control, ntree = 500, importance = TRUE)

# Model performance (average metrics across folds)
model_cv$results

# Confusion Matrix and performance metrics (based on all the cross-validated predictions)
confusionMatrix(model_cv$pred$pred, model_cv$pred$obs, positive = 'yes')
CM_Function(confusionMatrix(model_cv$pred$pred, model_cv$pred$obs, positive = 'yes'))

# Fit the Random Forest model with the optimal mtry value
RF_optimal <- randomForest(churn ~ ., data = training, mtry = 6, ntree = 500)

# Mean Decrease Gini
importance <- importance(RF_optimal)
importance
```