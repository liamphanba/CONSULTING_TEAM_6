---
title: "CASE_3"
author: "Liam Phan, Michael Bigler, Tania Loureiro, William Elkiess, Dakota Cuellar and Ilyana El Mendili"
date: "`r Sys.Date()`"
output: 
  rmdformats::downcute:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE,comment = FALSE, error = FALSE,options(scipen=999))
rm(list = ls())
```

# Packages

```{r}
library(DT)
library(adabag)
library(rpart.plot)
library(pROC)
library(summarytools)
library(corrplot)
library(dplyr)
library(GGally)
library(fastDummies)
library(ggcorrplot)
library(klaR)
library(psych)
library(MASS)
# library(ggord)
library(devtools)
library(ggplot2)
library(ggthemes)
library(GGally)
library(caret)
library(splitTools)
library(rpart)
library(xgboost)
library(caTools)
library(dplyr)
library(caret)
library(naniar)

CM_Function <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#2F4F4E')
  text(195, 435, 'No', cex=1.2)
  rect(250, 430, 340, 370, col='#0D8387')
  text(295, 435, 'Yes', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#0D8387')
  rect(250, 305, 340, 365, col='#2F4F4E')
  text(140, 400, 'No', cex=1.2, srt=90)
  text(140, 335, 'Yes', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  

#options(repos = c(
    #fawda123 = 'https://fawda123.r-universe.dev',
    #CRAN = 'https://cloud.r-project.org'))

# Install ggord
#install.packages('ggord')

```

# Data and Feature Engineering

```{r}

df <- readxl::read_xls('Cchurn.xls')
df$international_plan <- factor(df$international_plan, levels = c('no', 'yes'), labels = c('0','1'))
df$voice_mail_plan <- factor(df$voice_mail_plan, levels = c('no', 'yes'), labels = c('0','1'))
df$churn <- factor(df$churn, levels = c('no', 'yes'), labels = c('0','1'))

```

## Summary

```{r}
print(summarytools::dfSummary(df), method = 'render')
```

* We have no missing values -> perfect
* Heavily uneven counts of dependent variable (86 % no / 14 % yes) -> maybe sample for equality / maybe not because we loose information of other data
* Independent variables are on different scales -> standardize
* two (maybe three) categorical predictors: International plan / voice_mail_plan (/ maybe number_customer_service_calls) -> dummy encode -> not necessary as already 0 and 1
* Rest of data is numeric and most of the variables looks normally distributed with exception of number_vmail_messages and totat_intl_calls
  * transform these value to make them normal?
  * maybe make parts of them categorical? (recieving voice mail or not, calling internationally or not)
  * or maybe the categorical values that we have already give an indication for this
  * Test normality of variables
* Can variables be combined? We have day / eve / night / intl calls and for each of them minutes / calls / charge. Maybe we can combine this into one metric. Maybe average cost per minute or average cost per call? 

## Correlation Plot

```{r correlation}

df_numeric <- select_if(df, is.numeric)  # Subset numeric columns with dplyr

M <- cor(df_numeric)

p.mat <- cor_pmat(df_numeric)

ggcorrplot(M, hc.order = TRUE, type = "lower", lab = TRUE, p.mat = p.mat, sig.level=0.05, lab_size = 2, tl.cex = 10,outline.col = "white", ggtheme = ggplot2::theme_minimal(), colors = c("#823038", "white", "#2596be")) 

```

Proves theory from before -> we can make one metric out of charge and minutes --> charge / minutes

## Data Engineering

```{r}
df$total_day_charge_per_minute <- ifelse(df$total_day_minutes == 0, 0, df$total_day_charge / df$total_day_minutes)
df$total_eve_charge_per_minute <- ifelse(df$total_eve_minutes == 0, 0, df$total_eve_charge / df$total_eve_minutes)
df$total_night_charge_per_minute <- ifelse(df$total_night_minutes == 0, 0, df$total_night_charge / df$total_night_minutes)
df$total_intl_charge_per_minute <- ifelse(df$total_intl_minutes == 0, 0, df$total_intl_charge / df$total_intl_minutes)
df <- subset(df, select = -c(total_day_charge, total_day_minutes, total_eve_charge, total_eve_minutes, total_night_charge, total_night_minutes, total_intl_charge, total_intl_minutes))
```

## Correlation Plot

```{r}

df_numeric <- select_if(df, is.numeric)  # Subset numeric columns with dplyr

M <- cor(df_numeric)

p.mat <- cor_pmat(df_numeric)

ggcorrplot(M, hc.order = TRUE, type = "lower", lab = TRUE, p.mat = p.mat, sig.level=0.05, lab_size = 2, tl.cex = 10,outline.col = "white", ggtheme = ggplot2::theme_minimal(), colors = c("#823038", "white", "#2596be")) 

```

Now we have non-correlated data

## Relationship between variables

```{r}
# theme_set(theme_minimal())
# 
# ggpairs(
#   data = df,
#   columns = c(1:9,11:14),
#   mapping = aes(col = churn, alpha = .9)
# ) +
#   scale_fill_colorblind() +
#   scale_color_colorblind()
```

We see that data is hard to seperate linearly between the classes. Therefore one can introduce new features of higher order or use methods which do not need the data to be seperable linearly. 

## Adding features of higher order

Only squaring as we have no negative data. Cubing would be needed with negative data. 

```{r}
# squared
df2 <- df^2
df2 <- df2[,-c(2,3,10)]
colnames(df2) <- paste0(colnames(df2), '_sqd')

df <- cbind(df,df2)
```

## Relationship between data in higher order

```{r}
# theme_set(theme_minimal())
# 
# ggpairs(
#   data = df,
#   columns = c(1:9, 11:25),
#   mapping = aes(col = churn, alpha = .9)
# ) +
#   scale_fill_colorblind() +
#   scale_color_colorblind()
```

## Sampling Methods

As we have unbalanced data we need to use a sampling method to balance the classes. Hereby there are four different methods. OVER / UNDER / BOTH / ROSE. 

```{r}

library(ROSE)

# OVER
df_OVER <- ovun.sample(churn~., data = df, method = "over")$data

table(df$churn)
table(df_OVER$churn)

# UNDER
df_UNDER <- ovun.sample(churn~., data = df, method = "under")$data

table(df$churn)
table(df_UNDER$churn)

# BOTH
df_BOTH <- ovun.sample(churn~., data = df, method = "both")$data

table(df$churn)
table(df_BOTH$churn)

# ROSE
df_ROSE <- ROSE(churn ~ ., data = df, seed = 1, p = 0.5)$data

```

## Sampling Visualization

```{r}

# theme_set(theme_minimal())
# 
# ggpairs(
#   data = df_ROSE,
#   columns = c(1:9, 11:25),
#   mapping = aes(col = churn, alpha = .9)
# ) +
#   scale_fill_colorblind() +
#   scale_color_colorblind() +
#   labs(title = "Machine Learning Project")
# 
# ggpairs(
#   data = df_OVER,
#   columns = c(1:9, 11:25),
#   mapping = aes(col = churn, alpha = .9)
# ) +
#   scale_fill_colorblind() +
#   scale_color_colorblind() +
#   labs(title = "Machine Learning Project")

# ggpairs(
#   data = df_UNDER,
#   columns = c(1:9, 11:25),
#   mapping = aes(col = churn, alpha = .9)
# ) +
#   scale_fill_colorblind() +
#   scale_color_colorblind() +
#   labs(title = "Machine Learning Project")

# ggpairs(
#   data = df_BOTH,
#   columns = c(1:9, 11:25),
#   mapping = aes(col = churn, alpha = .9)
# ) +
#   scale_fill_colorblind() +
#   scale_color_colorblind() +
#   labs(title = "Machine Learning Project")
```

## Train / Test split

As we need to test the models we need to split the sampled data. 

```{r}
set.seed(1)
data <- df_OVER # choose which data to use df_ROSE / df_BOTH / df_UNDER / df_OVER / df
inds <- splitTools::partition(data$churn, p = c(train = 0.7, test = 0.3))
dftrain <- data[inds$train,]
dftest <- data[inds$test,]
```

## Standardizing

As some methods need scaled data we scale the data here to be centered. 

```{r}
norm.value <- preProcess(dftrain, method = c("center", "scale"))
dftrain <- predict(norm.value, dftrain)
dftest <- predict(norm.value, dftest)
```

# Predicting Models

## neural net

```{r}
# dftrain <- dftrain |> 
#   mutate_if(is.factor, as.character) |> 
#   mutate_if(is.character, as.numeric)
# 
# library(neuralnet)
# mod.neural <- neuralnet(churn ~ ., data = dftrain, hidden=c(15,15), linear.output = FALSE)
# 
# predicted.neural <- predict(mod.neural, dftest[,-c(10)])
# 
# confmat.neural <- confusionMatrix(data=predicted.neural, reference = dftest$churn, positive = '1')
# 
# CM_Function(confmat.neural)
# 
# roc_score.neural =roc(factor(dftest$churn, ordered=TRUE), factor(predicted.neural, ordered=TRUE))
# plot(roc_score.neural ,main ="ROC curve")

```

## Boosting

```{r}

set.seed(123)

# train bagged model
mod.boost <- boosting(churn ~., data=dftrain)

predicted.boost <- factor(predict(mod.boost, dftest, type="class")$class)

confmat.boost <- confusionMatrix(data=predicted.boost, reference = dftest$churn, positive = '1')

CM_Function(confmat.boost)

roc_score.boost =roc(factor(dftest$churn, ordered=TRUE), factor(predicted.boost, ordered=TRUE))
plot(roc_score.boost ,main ="ROC curve")

```

## Ctree

```{r}
tree_full <- rpart(churn ~ ., 
              data = dftrain, 
              method = "class",  # "class" because Y is a binary factor
              minbucket = 1,
              cp = 0.00001) 

# Plot tree
rpart.plot(tree_full, yesno = TRUE, digits =-6)

min_xerr<- which.min(tree_full$cptable[,"xerror"]) # select minimum cross-validation error
cp_bp <- tree_full$cptable[min_xerr,"CP"]  # find the corresponding CP value, to get the "best pruned " tree


mod.pruned_tree<- prune(tree_full, cp = cp_bp) # re-compute the tree with the selected Cp
rpart.plot(mod.pruned_tree, yesno = TRUE, digits =-3)

predicted.pruned_tree <- predict(mod.pruned_tree, dftest[,-c(10)], type = "class")

confmat.prunned_tree <- confusionMatrix(data=predicted.pruned_tree, reference = dftest$churn, positive = '1')

CM_Function(confmat.prunned_tree)

roc_score.prunned_tree =roc(factor(dftest$churn, ordered=TRUE), factor(predicted.pruned_tree, ordered=TRUE))
plot(roc_score.prunned_tree ,main ="ROC curve")
```

## Bagging 

```{r}
set.seed(123)
library(ipred)

# train bagged model
ames_bag1 <- bagging(
  formula = churn ~ .,
  data = dftrain, 
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
  )

ames_bag1

predicted <- factor(ifelse(predict(ames_bag1, dftest[,-c(10)], type = 'prob')[,2] >= 0.5, 1, 0))
                    
CM_Function(confusionMatrix(data=predicted, reference = dftest$churn, positive = '1'))

roc_score=roc(factor(dftest$churn, ordered=TRUE), factor(predicted, ordered=TRUE)) #AUC score
plot(roc_score ,main ="ROC curve")

```


## knn

```{r}

set.seed(1)

df <- data.frame(k = seq(1, 30, 1), accuracy = rep(0, 30), sensitivity = rep(0, 30))

# iterating over different ks
for(i in 1:30){
  # nearest neighbor
  KNN1 <- knn3(y = dftrain$churn, x = dftrain[,-c(10)], k = i)

  # predictions response 
  KNN1.pred.valid.resp <- predict(KNN1, dftest[,-c(10)], type = "class")
  
  # predictions prob 
  KNN1.pred.valid.prob <- predict(KNN1, dftest[,-c(10)], type = "prob")[,2]
  
  # Confusionmatrix
  df$sensitivity[i] <- confusionMatrix(KNN1.pred.valid.resp, dftest$churn, positive = "1")$byClass[1]
  df$accuracy[i] <- confusionMatrix(KNN1.pred.valid.resp, dftest$churn, positive = "1")$overall[1]

}

# plot the k's
ggplot(df, aes(x=k)) + 
  geom_line(aes(y = sensitivity, colour = "Sensitivity")) + 
  geom_line(aes(y = accuracy, colour = "Accuracy")) + 
  labs(x = "Number of k nearest neighbours", 
       y = "Accuracy / Sensitivity", title = "Accuracy / Sensitivity regarding k") +
  theme_minimal() + 
  scale_y_continuous(name = "Sensitivity / Accuracy", limits = c(0.7, 1)) +
    scale_color_manual(name = "Values", values = c("Sensitivity" = "darkblue", "Accuracy" = "red")) + 
  xlim (1, 30)


mod.knn <- knn3(y = dftrain$churn, x = dftrain[,-c(10)], k = 2)

predicted.knn <- predict(mod.knn, dftest[,-c(10)], type = "class")

confmat.knn <- confusionMatrix(data=predicted.knn, reference = dftest$churn, positive = '1')

CM_Function(confmat.knn)

roc_score.qda =roc(factor(dftest$churn, ordered=TRUE), factor(predicted.knn, ordered=TRUE))
plot(roc_score.qda ,main ="ROC curve")




```

## QDA

```{r}

mod.qda <- qda(churn ~., data = dftrain)

predicted.qda <- predict(mod.qda, dftest[,-c(10)])$class

confmat.qda <- confusionMatrix(data=predicted.qda, reference = dftest$churn, positive = '1')

CM_Function(confmat.qda)

roc_score.qda =roc(factor(dftest$churn, ordered=TRUE), factor(predicted.qda, ordered=TRUE))
plot(roc_score.qda ,main ="ROC curve")

```

## QLOG

```{r qda}

mod.log <- glm(churn ~., data = dftrain, family = binomial(link = "probit"))

s <- step(mod.log)

mod.log <- glm(s$formula, data = dftrain, family = binomial(link = "probit"))

predicted.log <- factor(ifelse(predict(mod.log, dftest[,-c(10)], type='response')>0.5,1,0))

confmat.log <- confusionMatrix(data=predicted.log, reference = dftest$churn, positive = '1')

CM_Function(confmat.log)

roc_score.log =roc(factor(dftest$churn, ordered=TRUE), factor(predicted.log, ordered=TRUE))
plot(roc_score.log ,main ="ROC curve")

```

## Gaussian SVM

```{r}
library(e1071)
mod.svm  = svm(formula = churn ~ .,
               data = dftrain,
                 type = 'C-classification', # this is because we want to make a regression classification
                 kernel = 'radial')

predicted.svm <- predict(mod.svm, dftest[,-c(10)])

confmat.svm <- confusionMatrix(data=predicted.svm, reference = dftest$churn, positive = '1')

CM_Function(confmat.svm)

roc_score.svm =roc(factor(dftest$churn, ordered=TRUE), factor(predicted.svm, ordered=TRUE))
plot(roc_score.svm ,main ="ROC curve")
```

## Boosting 

```{r}

# library(gbm)
# 
# mod <-  gbm(churn ~.,
#                 data = dftrain,
#                 distribution = "gaussian",
#                 cv.folds = 10,
#                 shrinkage = .01,
#                 n.minobsinnode = 10,
#                 n.trees = 500)
# 
# predicted <- factor(ifelse(1/(1+exp(-2*predict.gbm(mod, dftest[,-c(10)])))>=0.5,1,0))
# 
# 
# dftrain <-  dftrain |>
#   mutate_if(is.factor, as.character) |>
#   mutate_if(is.character, as.numeric)
# 
# dftest <-  dftest |>
#   mutate_if(is.factor, as.character) |>
#   mutate_if(is.character, as.numeric)
# 
# xgb_train <- xgb.DMatrix(data = as.matrix(dftrain[,-c(10)]), label = dftrain$churn)
# xgb_test <- xgb.DMatrix(data = as.matrix(dftest[,-c(10)]), label = dftest$churn)
# xgb_params <- list(
#   booster = "gbtree",
#   eta = 0.01,
#   max_depth = 8,
#   gamma = 4,
#   subsample = 0.75,
#   colsample_bytree = 1,
#   objective = "multi:softprob",
#   eval_metric = "mlogloss",
#   num_class = 2)
# 
# xgb_model <- xgb.train(
#   params = xgb_params,
#   data = xgb_train,
#   nrounds = 100,
#   verbose = 1
# )
# 
# xgb_model
# 
# xgb_preds <- predict(xgb_model, as.matrix(dftest$churn), reshape = TRUE)
# xgb_preds <- as.data.frame(xgb_preds)
# colnames(xgb_preds) <- c(0,1)
# predicted <- ifelse(xgb_preds[,2] > 0.5, 1, 0)
# 
# CM_Function(confusionMatrix(data=predicted, reference = dftest$churn, positive = "1"))
# 
# library(pROC)
# roc_score=roc(factor(dftest$churn, ordered=TRUE), factor(predicted, ordered=TRUE)) #AUC score
# plot(roc_score ,main ="ROC curve")

```


##  Logistic Regression

```{r}

#mod <- glm(churn ~., data = df_UNDER, family = binomial(link='logit'))
#summary(mod)

#churn_predicted <- factor(ifelse(predict(mod, df[,-c(10)], type = 'response') < 0.5, 'no', 'yes'))

#library(caret)

#LR_Confusion_Matrix_training <- confusionMatrix(data=churn_predicted, reference = df$churn, positive = 'yes')

#CM_Function(LR_Confusion_Matrix_training)

```

Log regression doesn't work. Changing the decision boundary also doesn't help. --> maybe we actually need to resample

https://www.r-bloggers.com/2021/05/class-imbalance-handling-imbalanced-data-in-r/
https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/
https://datascientistdiary.com/index.php/2021/09/02/how-to-handle-imbalanced-data-example-in-r/
https://cran.r-project.org/web/packages/imbalance/vignettes/imbalance.pdf


## LDA 

```{r}

# LDA Model Fit on Training
#LDA_training <- lda(churn~., df_UNDER)
#LDA_training

# Density Plot for Overlapping 
#p <- predict(LDA_training, df)
#ldahist(data = p$x[,1], g = df$churn, col = "#0D8387")

```

### Confusion Matrix (Training VS Test)

```{r}

# Training 60%
#LDA_predictions_training <- predict(LDA_training, df)$class

#LDA_Confusion_Matrix_training <- confusionMatrix(data = LDA_predictions_training, reference = df$churn, positive='yes')

#CM_Function(LDA_Confusion_Matrix_training)

```

## Random Forest V2

```{r}

# ROSE DATA

#rf <- randomForest(churn~., data=df_UNDER, proximity=TRUE)


#Predictions_OVER <- predict(rf, df_UNDER[,-10])
#CM_OVER <- confusionMatrix(Predictions_OVER, df_UNDER$churn, positive = 'yes')

#CM_Function(CM_OVER)

# ORIGINAL DATA

#Predictions_DATA <- predict(rf, df[,-10])

#CM_DATA <- confusionMatrix(Predictions_DATA, df$churn, positive = 'yes')

#CM_Function(CM_DATA)

```


## Decision Trees

```{r}

#tree <- rpart(churn ~., data = df)

#rpart.plot(tree)

#printcp(tree)
#plotcp(tree)

#Predictions_DT <- predict(tree, df[,-10])[,2]

#Predictions_DT <- ifelse(Predictions_DT > 0.5, "yes","no")

#Predictions_DT <- as.factor(Predictions_DT)

#CM_DATA <- confusionMatrix(Predictions_DT, df$churn, positive = 'yes')

#CM_Function(CM_DATA)

```

## Neural Net

```{r}

#library(caret)

#nn1 <- train(churn ~ ., data = df_UNDER, method = "nnet")

#nn1.pre <- predict(nn1, df[,-c(10)])

#confusionMatrix(nn1.pre, df$churn, positive = 'yes')
#CM_Function(confusionMatrix(nn1.pre, df$churn, positive = 'yes'))

```


## Support Vector Machines

```{r}

#library(e1071)

#classifierR = svm(formula = churn ~ .,
                 #data = dftrain,
                 #type = 'C-classification', # this is because we want to make a regression classification
                 #kernel = 'radial', 
                 #cost = 100,
                 #gamma = 20)


#svm_1 <- predict(object = classifierR, newdata = dftest[,-c(10)])

#confusionMatrix(svm_1, dftest$churn, positive = '1')

#CM_Function(confusionMatrix(svm_1, df$churn, positive = '1'))

```